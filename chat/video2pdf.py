import numpy as np
from openai import OpenAI
import time
import json
import google.generativeai as genai
import io
import base64
from PIL import Image, ImageDraw, ImageFont, ImageOps
import cv2
import requests
from fpdf import FPDF
import fitz  # PyMuPDF
from io import BytesIO


def get_PIL_from_pdf(pdf_path):
    pdf_document = fitz.open(pdf_path)

    pil_images = []
    for page_number in range(len(pdf_document)):
        # Get the page
        page = pdf_document[page_number]

        # Render the page to a pixmap
        pix = page.get_pixmap()

        # Convert the pixmap to a bytes object
        img_bytes = pix.tobytes("png")

        # Convert the bytes object to a PIL Image
        pil_image = Image.open(BytesIO(img_bytes))
        pil_images.append(pil_image)
    return pil_images


def bytes_from_im(img):
    is_success, buffer = cv2.imencode(".jpg", img)
    io_buf = io.BytesIO(buffer)
    # decode
    decode_img = cv2.imdecode(np.frombuffer(io_buf.getbuffer(), np.uint8), -1)
    return base64.b64encode(buffer).decode("utf-8")


def get_request(prompt, images):
    buffers = [bytes_from_im(img) for img in images]
    btes_images = [bytes_from_im(img) for img in images]
    image_requests = [
        [
            {"type": "text", "text": prompt + f"{i}"},
            {
                "type": "image_url",
                "image_url": {"url": f"data:image/jpeg;base64,{btes}"},
            },
        ]
        for i, btes in enumerate(btes_images)
    ]
    return image_requests


def get_timestamps(model, video_path):
    file = genai.upload_file(video_path)
    # Videos need to be processed before you can use them.
    while file.state.name == "PROCESSING":
        print("processing video...")
        time.sleep(5)
        file = genai.get_file(file.name)

    prompt = """Iâ€™m trying to make a comic strip that summarizes this video. Give me key timestamps in MM:SS of the video from which I can draw the comic panels.\
  The JSON format you should output is as follows: a list  of [ { \"timestamp\": \"MM:SS\", \"reason\": reason_for_timestamp,\
    \"panel_dialogue\": string dialogue that is relevant to the panel at that timestamp} ]

    Only output the json string without any other text. Make the output parseable in Python"""
    # "Explain this video clip in a series of comic strip panel descriptions. Include important quotes in the descriptions. The quotes must contain less than 5 words. Also include important visual details. In a JSON format--a list of {\"panel\": panel_number, \"description\": description_with_key_dialogue}."
    result = model.generate_content([file, prompt])
    # print(result.text)
    try:
        final_json = json.loads(result.text[8:-4])
    except:
        final_json = json.loads(result.text)
    return final_json


def get_frame_at_timestamp(video_path, timestamp):
    # Load the video
    cap = cv2.VideoCapture(video_path)

    # Check if the video was loaded successfully
    if not cap.isOpened():
        print("Error: Could not open the video.")
        exit()

    # Calculate the frame number based on the video's frames per second (fps)
    fps = cap.get(cv2.CAP_PROP_FPS)
    frame_number = int(fps * timestamp)

    frames = []

    for i in range(-3, 3, 1):
        # Set the video to the calculated frame number
        cap.set(cv2.CAP_PROP_POS_FRAMES, max(0, frame_number + i * int(fps / 4)))

        # Read the frame
        ret, frame = cap.read()
        frames.append(frame)

    # Release the video capture
    cap.release()
    return frames


def mmss_to_seconds(mmss):
    minutes, seconds = map(int, mmss.split(":"))
    return minutes * 60 + seconds


def identify_keyframe(client, prompt, request):
    out = client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "system", "content": prompt}]
        + [{"role": "user", "content": im_req} for im_req in request],
        temperature=0.000001,
    )
    topics = out.choices[0].message.content
    return topics


def get_video_frames(client, video_path, text_json, offset=0.5):
    final_frames = []
    prompt_keyframe = """I will give you a number of frames from a small segment of a video. I need you to identify the best-looking frame out of all of these \
so I can use it as a basis for a comic book panel created from the video. The frame needs to have no blur. Only output the number of the best frame, as an integer without any additional words."""
    for panel in text_json:
        timestamp = mmss_to_seconds(panel["timestamp"])
        description = panel["panel_dialogue"]
        frames = get_frame_at_timestamp(video_path, timestamp + offset)
        req = get_request(
            f"Here is one of the frames you need to analyze. For some context, this is the dialogue that is attached to the picture: {description}. This is frame number ",
            frames,
        )
        frame_id = int(identify_keyframe(client, prompt_keyframe, req))
        frame = frames[frame_id]
        final_frames.append((frame[..., ::-1], description))
    return final_frames


def get_boxes(model, image):
    results = model(image)[0].boxes
    people = []
    for i, cls in enumerate(results.cls):
        if results.cls[i] == 0:
            people.append(list(np.array(results.xyxy[i])))
    return np.array(people)


def get_center_person(model, image, frame):
    results = get_boxes(model, image)
    if len(results) == 0:
        return None
    centers = (results[:, :2] + results[:, 2:]) / 2
    centers = centers - (np.array(frame.shape)[:2] / 2)
    centers_dist = np.linalg.norm(centers, axis=1)
    centered = np.argmin(centers_dist)
    box = results[centered]
    return box


def draw_text_bubble(model_yolo, frame, img, desc):
    center_box = get_center_person(model_yolo, frame, frame)

    factor_x = frame.shape[1] / img.size[0]
    factor_y = frame.shape[0] / img.size[1]

    img_width = img.size[0]
    max_width = img_width * 0.5
    font_size = int(max_width / 9.5)
    font = ImageFont.load_default(size=font_size)

    if (not (center_box is None)) and (desc is not None):
        draw = ImageDraw.Draw(img)
        x1, y1, x2, y2 = center_box
        y_top = min(y1, y2) / factor_y
        x_mid = (x1 + x2) / 2
        x_mid /= factor_x

        # Get text sizes
        lines = [""]
        line_lengths = []
        line_length = 0
        for word in desc.split():
            lines[-1] += " " + word
            line_length = draw.textlength(lines[-1], font=font)
            if line_length > max_width:
                line_lengths.append(line_length)
                lines.append("")
        line_lengths.append(line_length)

        width = max(line_lengths)
        height = font_size * len(lines)

        # Get position/size of the bounding speech bubble
        text_box = [x_mid, y_top - height, x_mid + width, y_top]

        right_overflow = text_box[2] - frame.shape[1] / factor_x
        if right_overflow > 0:
            text_box[0] -= right_overflow + 50
            text_box[2] -= right_overflow + 50

        top_overflow = 0 - text_box[3]
        if top_overflow > 0:
            text_box[1] += top_overflow * 1.2
            text_box[3] += top_overflow * 1.2

        # draw speech bubble
        draw.rounded_rectangle(
            [
                text_box[0] - 10,
                text_box[1],
                text_box[0] - 10 + width * 1.1,
                text_box[1] + height * 1.1,
            ],
            fill="white",
            outline="black",
            radius=15,
            width=5,
        )

        # draw text inside speech bubble
        for i, line in enumerate(lines):
            draw.text(
                (text_box[0], text_box[1] + i * font_size * 1.05),
                line,
                font=font,
                fill="black",
            )
    return img


def cartoonify(frame, STYLE_API):
    MAGIC_API_KEY = STYLE_API
    cv2.imwrite("frame.png", frame)
    url = "https://api.magicapi.dev/api/v1/ailabtools/ai-cartoon-generator/image/effects/generate_cartoonized_image"
    headers = {"x-magicapi-key": MAGIC_API_KEY}
    files = {
        "task_type": (None, "async"),
        "image": ("frame.png", open("frame.png", "rb"), "image/png"),
        "index": (None, "0"),  # style index (see docs)
    }

    response = requests.post(url, headers=headers, files=files)
    response_dict = response.json()
    task_id = response_dict.get("task_id")
    print(response.text)

    url = f"https://api.magicapi.dev/api/v1/ailabtools/ai-cartoon-generator/api/apimarket/query-async-task-result?task_id={task_id}"
    headers = {"x-magicapi-key": MAGIC_API_KEY}
    task_status = 1
    while task_status != 2:  # wait until status = 2 (completed)
        response = requests.get(url, headers=headers)
        response_dict = response.json()
        print(response_dict)
        task_status = response_dict.get("task_status", 0)
        # print(response.text)
        if task_status != 2:
            print(task_status)
            time.sleep(1)

    print("Processing completed, handling image...")
    image_url = response_dict.get("data", {}).get("result_url")

    # download
    image_response = requests.get(image_url)
    img_data = np.frombuffer(image_response.content, np.uint8)
    return cv2.imdecode(img_data, cv2.IMREAD_COLOR)[..., ::-1]


def add_boundaries(img):
    # Original image dimensions
    original_width, original_height = img.size

    # Calculate 5% black boundary size
    black_border_size = int(0.03 * original_width), int(0.03 * original_height)

    # Add black border
    img_with_black_border = ImageOps.expand(img, border=black_border_size, fill="black")

    # Calculate new dimensions after black border
    new_width, new_height = img_with_black_border.size

    # Calculate 10% white boundary based on new dimensions
    white_border_size = int(0.06 * new_width), int(0.06 * new_height)

    # Add white border
    final_img = ImageOps.expand(
        img_with_black_border, border=white_border_size, fill="white"
    )

    return final_img


def resize_image(img, max_width, target_height):
    """
    Resize the image to have the specified target height while maintaining aspect ratio.
    If the resulting width exceeds max_width, resize again to fit within max_width.
    """
    width, height = img.size

    # Calculate new dimensions maintaining aspect ratio
    aspect_ratio = width / height
    new_width = int(target_height * aspect_ratio)
    new_height = target_height

    # Resize if necessary
    if new_width > max_width:
        new_width = max_width
        new_height = int(max_width / aspect_ratio)

    img = img.resize((new_width, new_height), Image.Resampling.LANCZOS)
    return img


def create_comic_pdf(images, output_pdf):
    # PDF dimensions
    pdf_width, pdf_height = 816, 1056
    image_height = int(0.45 * pdf_height)  # 40% of the PDF height

    # Initialize PDF
    pdf = FPDF(unit="pt", format=(pdf_width, pdf_height))
    pdf.set_auto_page_break(auto=True, margin=0)

    # Track position on the page
    x, y = 0, 0

    for img_ in images:
        # Resize the image to fit the constraints
        img = resize_image(img_, max_width=pdf_width, target_height=image_height)

        # Update the x, y position for the next image
        if x + img.width > pdf_width:
            x = 0
            y += image_height

        # Check if we need a new page
        if y + image_height > pdf_height:
            # Reset position for new page
            x, y = 0, 0
        # Add new page if we're at the top left
        if x == 0 and y == 0:
            pdf.add_page()
        # Add image to PDF
        pdf.image(img, x=x, y=y)

        x += img.width

    # Save the final PDF
    pdf.output(output_pdf)


def get_pdf(
    video_path, OPENAI_API, GEMINI_API, STYLE_API, output_path="comic_output.pdf"
):
    from ultralytics import YOLO

    model_yolo = YOLO("yolov9c.pt")
    model = genai.GenerativeModel("gemini-1.5-pro")

    client = OpenAI(api_key=OPENAI_API)
    genai.configure(api_key=GEMINI_API)

    text_json = get_timestamps(model, video_path)

    frames = get_video_frames(client, video_path, text_json, offset=1)

    final_imgs = []
    for frame, desc in frames:
        img = Image.fromarray(cartoonify(frame, STYLE_API))
        img = draw_text_bubble(model_yolo, frame, img, desc)

        final_imgs.append(img)

    for i, img in enumerate(final_imgs):
        boundary_img = add_boundaries(img)
        final_imgs[i] = boundary_img

    create_comic_pdf(final_imgs, output_path)
